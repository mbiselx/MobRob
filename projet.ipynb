{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Basics of Mobile Robotics Project\n",
    "\n",
    "## Index\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> <img src=\"./imgs/pdf-based_lacation_estimate.png\" alt=\"pdf-based_lacation_estimate\" style=\"width: 400px;\"/>\n",
    "\n",
    "> *Here is an example of the image we are working with :*\n",
    "\n",
    "# The image processing can be divided into four steps :\n",
    "\n",
    "### 1) Obtain an image of the region of interest : the map of the envirronment.\n",
    "We want to get rid off all the random components introduced by taking a picture. it's include translations, rotations, scaling and most importantly perspective transformations. Therefore we want to search four points, which we willingly place at known distances and angles from each other, to standardize our image. \n",
    "\n",
    "The best is to detect the four extreme angles of the rectangle environment. It's increases the precision of the transformation by decreasing the relative error on their distance to each other. There are several ways to find these corner : \n",
    "\n",
    "- detect the lines of the rectangle (hough transform or edge detection) but there will be to many line with obstacles. It could have been possible to detect the edges of the outer rectangle defining the map, fill it (meaning put all the pixels inside its border to one), next erode the image to delete all exterieur components to the rectangle, our region of interest. Then we just have to dilate again to have only a big rectangle of white pixels on a black background, which is very easy to detect. But our backround had too much long lines throughout the whole image (for example foots of the table) then filling closed borders would have lead to fill the entire image. To summarize, this is not a good method because our region of interest does not produce the biggest closed border.\n",
    "- convolve the image with a corner (two small lines 90 degrees to each others) has the advantage of being quite scale invariant but depends on rotation. The results were very bad as the image often presents orientations. Furthemore with image with grid pattern we can't distinguish all the corners.\n",
    "- convolve with a disk has the advantage of being rotation invariant, which is better than scale invariant. So we introduced specific patterns at the corners to inderictly detect them. Its better to have disk with specific color, but even then convolution was not good enough (with perspective, disk become ellipsoids and are not detected at all).\n",
    "-use the HSV (Hue Saturation Value) representation of the image to easily detect colors. We pass from RGB representation where it's hard to detect specific colors to the HSV where saturated colors (meaning very red, very blue or very green) are easy to detect on a neutral backgound. Therefore we just need colors, without specific shape. We kept the disks because they are more robust to bad detection : when some pixels are not detect they mostly stay as one connected region of pixels with the same centroid.\n",
    "We chose the last option, the most efficient.\n",
    "\n",
    "\n",
    "\n",
    "0) just display the orginal image.\n",
    "\n",
    "1) Obtain an image of the region of interest : the map of the envirronment.\n",
    "\n",
    "2) Find the position of the robot.\n",
    "\n",
    "3) Find the goal\n",
    "\n",
    "4) get a map for the global path planning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import libraries and define usefull functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from skimage import morphology\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "np.random.seed(0) # To guarantee the same outcome on all computers\n",
    "\n",
    "\n",
    "#variable definition\n",
    "corner= np.zeros((4,2),np.uint32)  #matrix of the detected patterns' centers. int because thee are index\n",
    "corner_sort = np.zeros((4,2),np.uint32)  #sorted matrix of the detected patterns' centers\n",
    "distancex = 1200  #true distance of the robot environment\n",
    "distancey =  800  #true distance of the robot environment\n",
    "start = np.zeros(3,np.float32) #starting point of ther robot (x,y,angle). float to compute the angle\n",
    "end = np.zeros(2,np.int32) #goal point (x,y)\n",
    "centroid = np.zeros((2,3),np.int32) #centroids of the red disk and their number of points\n",
    "robot_size_kernel = (15,15)#kernel of the size of the robot in pixel in the resized map\n",
    "capt_point = (850,700)#point at which we emulate the sensing of an onboard camera\n",
    "capt_xdim = 200#dimension of the simulated camera view\n",
    "capt_ydim = 200\n",
    "img_path = 'photos_test_3\\IMG_5939.JPG'#if we take the image from the computer\n",
    "\n",
    "goalReached = -100#is goal reached? terminate the program\n",
    "capt_xdim = 200 #dimension of the emulated view, the bigger it is, the more the precision of the postion is\n",
    "capt_ydim = 200\n",
    "true_robot_length = 20#length between the center of the robot and a bit forward in pixels in the true map\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#function definition\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "#Image processing functions__START\n",
    "##################################################################################################\n",
    "def stackImages(scale, imgArray):\n",
    "    rows = len(imgArray)\n",
    "    cols = len(imgArray[0])\n",
    "    rowsAvailable = isinstance(imgArray[0], list)\n",
    "    width = imgArray[0][0].shape[1]\n",
    "    height = imgArray[0][0].shape[0]\n",
    "    if rowsAvailable:\n",
    "        for x in range(0, rows):\n",
    "            for y in range(0, cols):\n",
    "                if imgArray[x][y].shape[:2] == imgArray[0][0].shape[:2]:\n",
    "                    imgArray[x][y] = cv2.resize(imgArray[x][y], (0, 0), None, scale, scale)\n",
    "                else:\n",
    "                    imgArray[x][y] = cv2.resize(imgArray[x][y], (imgArray[0][0].shape[1], imgArray[0][0].shape[0]),\n",
    "                                                None, scale, scale)\n",
    "                if len(imgArray[x][y].shape) == 2: imgArray[x][y] = cv2.cvtColor(imgArray[x][y], cv2.COLOR_GRAY2BGR)\n",
    "        imageBlank = np.zeros((height, width, 3), np.uint8)\n",
    "        hor = [imageBlank] * rows\n",
    "        hor_con = [imageBlank] * rows\n",
    "        for x in range(0, rows):\n",
    "            hor[x] = np.hstack(imgArray[x])\n",
    "        ver = np.vstack(hor)\n",
    "    else:\n",
    "        for x in range(0, rows):\n",
    "            if imgArray[x].shape[:2] == imgArray[0].shape[:2]:\n",
    "                imgArray[x] = cv2.resize(imgArray[x], (0, 0), None, scale, scale)\n",
    "            else:\n",
    "                imgArray[x] = cv2.resize(imgArray[x], (imgArray[0].shape[1], imgArray[0].shape[0]), None, scale, scale)\n",
    "            if len(imgArray[x].shape) == 2: imgArray[x] = cv2.cvtColor(imgArray[x], cv2.COLOR_GRAY2BGR)\n",
    "        hor = np.hstack(imgArray)\n",
    "        ver = hor\n",
    "    return ver\n",
    "\n",
    "def display(output, fact = 0.3):\n",
    "    cv2.namedWindow(\"output\", cv2.WINDOW_NORMAL )  # Create window with freedom of dimensions\n",
    "    cv2.resizeWindow(\"output\", math.floor ( fact * output.shape [1] ) , math.floor (fact * output.shape [0] ) )  # Resize window to specified dimensions keeping the image form\n",
    "    cv2.imshow(\"output\", output)\n",
    "    cv2.waitKey ( 0 )\n",
    "    \n",
    "def angle(x0,y0,x1,y1):\n",
    "    #directly put the angle in the start array\n",
    "    dist = math.sqrt ( (x0 - x1)*(x0 - x1) + (y0 - y1)*(y0 - y1)  )\n",
    "    x = (x1 - x0)/dist#normalisation required\n",
    "    y = (y1 - y0)/dist\n",
    "    start[2] = math.acos(x)\n",
    "    \n",
    "def prepare_image(path):\n",
    "    #read the image saved in path and return it in RGB mode\n",
    "    img = cv2.imread(path)\n",
    "    u,v,d = img.shape\n",
    "    if (u>v):#if needed rotate the image to have a horizontal one\n",
    "        img = cv2.rotate(img,cv2.ROTATE_90_CLOCKWISE)\n",
    "    imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    return img\n",
    "\n",
    "def get_clean_map(img) :\n",
    "    #find the blue corner, make them wite, and map the image to get a nice map\n",
    "    #return true_map, the map in high resolution\n",
    "\n",
    "    #mask to get the blue color\n",
    "    imgHSV = cv2.cvtColor ( img , cv2.COLOR_BGR2HSV )\n",
    "    [h_min, s_min, v_min] = [90,138,108]\n",
    "    [h_max, s_max, v_max] = [130,255,255]\n",
    "    lower = np.array([h_min, s_min, v_min])\n",
    "    upper = np.array([h_max, s_max, v_max])\n",
    "    mask = cv2.inRange(imgHSV, lower, upper)\n",
    "\n",
    "    #for robustness and avoid useless labeling, remove all the noisy blue isolated pixels\n",
    "    img_dilate = cv2.dilate ( mask , np.ones ( (10 , 10) , np.uint8 ) , iterations=1 )\n",
    "    img_erode = cv2.erode(img_dilate,np.ones((10,10),np.uint8),iterations = 2)\n",
    "    img_dilate = cv2.dilate(img_erode,np.ones((10,10),np.uint8),iterations = 1)\n",
    "\n",
    "    #put the same number to all connected pixels:\n",
    "    labels = morphology.label(img_dilate, background=0)\n",
    "    #seek for the four biggest blobs i.e. the four labels with the most of pixels\n",
    "    #It's a robustness mesure : the four corners are supposed to have the same size and being much bigger than not wanted regions\n",
    "    unique_labels = (np.unique(labels))[1:]\n",
    "    comptage = np.zeros(unique_labels.size+1)\n",
    "    for i in unique_labels :\n",
    "        comptage[i] = np.sum(labels==i)\n",
    "    indice = np.argsort(comptage)\n",
    "    error =0\n",
    "    if (np.max(indice)<4):\n",
    "        return img\n",
    "    for i in range(0,4):#take the four biggest labels\n",
    "        ett_current = np.where(labels==(indice[indice.size-1-i]),1,0)\n",
    "        #erase the blue pixels not to detect them again\n",
    "        img [: , : , 0] = np.where ( labels == (indice [indice.size - 1 - i]) , 200 , img [: , : , 0] )\n",
    "        img [: , : , 1] = np.where ( labels == (indice [indice.size - 1 - i]) , 200 , img [: , : , 1] )\n",
    "        img [: , : , 2] = np.where ( labels == (indice [indice.size - 1 - i]) , 200 , img [: , : , 2] )\n",
    "        #compute the centroids of each blob, they are the corner of the map\n",
    "        ett_current = np.float32(ett_current)\n",
    "        M = cv2.moments (ett_current)\n",
    "        cX =  int ( M [\"m10\"] / M [\"m00\"] )\n",
    "        cY =  int ( M [\"m01\"] / M [\"m00\"] )\n",
    "        corner[i , :] = np.array([cX,cY])\n",
    "\n",
    "    #sort the corner\n",
    "    x_max = max(corner[:,0])\n",
    "    x_min = min(corner[:,0])\n",
    "    y_max = max(corner[:,1])\n",
    "    y_min = min(corner[:,1])\n",
    "    x_middle = (x_max-x_min)/2+x_min\n",
    "    y_middle = (y_max-y_min)/2+y_min\n",
    "    for i in range(0,4):\n",
    "        if (corner[i,0]<x_middle) & (corner[i,1]<y_middle):\n",
    "            corner_sort[0,:]=corner[i,:]\n",
    "        if (corner [i , 0] < x_middle) &  (corner [i , 1] > y_middle) :\n",
    "            corner_sort [1 , :] = corner [i , :]\n",
    "        if (corner [i , 0] > x_middle) &  (corner [i , 1] < y_middle) :\n",
    "            corner_sort [2 , :] = corner [i , :]\n",
    "        if (corner [i , 0] > x_middle) &  (corner [i , 1] > y_middle) :\n",
    "            corner_sort [3 , :] = corner [i , :]\n",
    "    #apply resizing\n",
    "    corner_goal = np.array([[0,0],[0,distancey], [distancex,0], [distancex,distancey]], np.float32)\n",
    "    M = cv2.getPerspectiveTransform(np.float32(corner_sort),corner_goal)\n",
    "    img_reconstruct = cv2.warpPerspective(img, M,(distancex,distancey))\n",
    "    true_map = img_reconstruct\n",
    "    return true_map\n",
    "\n",
    "def robot_detection(map):\n",
    "    #return the position of the robot (x,y,angle) angle between the direction where the robot is going and the lower abscisse axis\n",
    "\n",
    "    #filter red component from the map\n",
    "    imgHSV2 = cv2.cvtColor ( map, cv2.COLOR_BGR2HSV )\n",
    "    [h_min , s_min , v_min] = [0 , 160 , 0]\n",
    "    [h_max , s_max , v_max] = [180 , 255 , 255]\n",
    "    lower = np.array ( [h_min , s_min , v_min] )\n",
    "    upper = np.array ( [h_max , s_max , v_max] )\n",
    "    mask = cv2.inRange ( imgHSV2 , lower , upper )\n",
    "\n",
    "    # detect the big and the small blob\n",
    "    img_erode = cv2.erode ( mask , np.ones ( (10 , 10) , np.uint8 ) , iterations=1 )\n",
    "    img_dilate = cv2.dilate ( img_erode , np.ones ( (10 , 10) , np.uint8 ) , iterations=1 )\n",
    "    labels = morphology.label ( img_dilate , background=0 )\n",
    "    unique_labels = (np.unique ( labels )) [1 :]\n",
    "    comptage = np.zeros ( unique_labels.size + 1 )\n",
    "    for i in unique_labels :\n",
    "        comptage [i] = np.sum ( labels == i )\n",
    "    indice = np.argsort ( comptage )\n",
    "    \n",
    "    #plt.imshow(img_dilate)\n",
    "    \n",
    "    for i in range ( 0 , 2 ) :  # take the 2 biggest labels\n",
    "        ett_current = np.where ( labels == (indice [indice.size - 1 - i]) , 1 , 0 )\n",
    "        ett_current = np.float32 ( ett_current )\n",
    "        sum = np.sum ( ett_current )\n",
    "        M = cv2.moments ( ett_current )\n",
    "        cX = int ( M [\"m10\"] / M [\"m00\"] )\n",
    "        cY = int ( M [\"m01\"] / M [\"m00\"] )\n",
    "        centroid [i , :] = np.array ( [cX , cY , sum] )\n",
    "    #The biggest blob give the position (x,y) of the robot, we use also the small one to compute the angle\n",
    "    imax = np.argmax ( centroid [: , 2] )\n",
    "    imin = 1 - imax\n",
    "    start [0 :2] = centroid [imax , 0 :2]\n",
    "    angle ( centroid [imax, 0], centroid [imax, 1] , centroid [ imin, 0] ,centroid [imin,1]  )\n",
    "    \n",
    "    # Fix 3D issues & transform coords\n",
    "    start[0] = start[0] - 21/400*(start[0] - distancex/2)   # correct for non-verticaltiy: estimated empirical values\n",
    "    start[1] = distancey - start[1]                         # flip y dimension\n",
    "    start[1] = start[1] - 31/600*(start[1] + 200)           # even more guesswork\n",
    "    \n",
    "    return start\n",
    "\n",
    "def goal_detection(img_reconstruct):\n",
    "    # detect the goal point\n",
    "\n",
    "    #filter the green component\n",
    "    imgHSV3 = cv2.cvtColor ( img_reconstruct , cv2.COLOR_BGR2HSV )\n",
    "    [h_min , s_min , v_min] = [47 , 89 , 51]\n",
    "    [h_max , s_max , v_max] = [116 , 209 , 198]\n",
    "    lower = np.array ( [h_min , s_min , v_min] )\n",
    "    upper = np.array ( [h_max , s_max , v_max] )\n",
    "    mask = cv2.inRange ( imgHSV3 , lower , upper )\n",
    "\n",
    "\n",
    "    #Here we consider there are not to much not wanted pixel in the mask\n",
    "    #so we consider after erosion the only pixels to one are those from the goal\n",
    "    img_erode = cv2.erode ( mask , np.ones ( (15 , 15) , np.uint8 ) , iterations=1 )\n",
    "    img_dilate = cv2.dilate ( img_erode , np.ones ( (15 , 15) , np.uint8 ) , iterations=1 )\n",
    "\n",
    "    M = cv2.moments ( img_dilate )\n",
    "    cX = int ( M [\"m10\"] / M [\"m00\"] )\n",
    "    cY = int ( M [\"m01\"] / M [\"m00\"] )\n",
    "    end = np.array ( [cX , cY] )\n",
    "    \n",
    "    #fix coords\n",
    "    end[1] = distancey - end[1]\n",
    "    \n",
    "    return end\n",
    "\n",
    "def get_global_search_map(for_global_search):\n",
    "    #return the map (meaning contains only obstacles) dilated to make obstacles a bit bigger\n",
    "    #to take into account the size of the robot\n",
    "\n",
    "    #here we get ride of all thin black lines used for online positioning\n",
    "    map_gray = cv2.cvtColor ( for_global_search , cv2.COLOR_BGR2GRAY )\n",
    "    map_dilate = cv2.dilate ( map_gray , np.ones ( (4 , 4) , np.uint8 ) , iterations=1 )\n",
    "    map_erode = cv2.erode ( map_dilate , np.ones ( (4 , 4) , np.uint8 ) , iterations=1 )\n",
    "\n",
    "    ret , map_bin = cv2.threshold ( map_erode , 50 , 255 , cv2.THRESH_BINARY )#apply a fixed threshold binarization\n",
    "    map_bin = 255 - map_bin\n",
    "    #increase the size of the obstacles\n",
    "    map_global_search = cv2.dilate ( map_bin , np.ones ( robot_size_kernel , np.uint8 ) , iterations=1 )\n",
    "    return map_global_search\n",
    "\n",
    "def camera_emulation(true_map) :\n",
    "    # emulate onboard camera return the emulated view, a rectangle a bit in front of the detected robot position\n",
    "    # detect robot's position\n",
    "\n",
    "    pos = robot_detection ( true_map )  # detect the position of the robot on the map\n",
    "\n",
    "    # crop the image to have a small rectangle in front of the robot\n",
    "    capt_point = np.array ( [pos [1] - capt_ydim , pos [0]] ) + true_robot_length * np.array (\n",
    "        [-math.sin ( pos [2] ) , +math.cos ( pos [2] )] )  # the bottom left point of the emulated view\n",
    "    capt_point = np.int32 ( capt_point )\n",
    "    capt_rect = true_map [capt_point [0] : capt_point [0] + capt_xdim , capt_point [1] : capt_point [1] + capt_ydim]\n",
    "    # display the filmed region by highlighting it\n",
    "    true_map [capt_point [0] : capt_point [0] + capt_xdim , capt_point [1] : capt_point [1] + capt_ydim] = true_map [capt_point [0] : capt_point [0] + capt_xdim , capt_point [1] :capt_point [1] + capt_ydim] + 40\n",
    "\n",
    "    # get the probable position of the robot by convolving the emulated view with the map\n",
    "    res = cv2.matchTemplate ( true_map , capt_rect , eval ( 'cv2.TM_CCOEFF_NORMED' ) )\n",
    "    min_val , max_val , min_loc , max_loc = cv2.minMaxLoc ( res )\n",
    "\n",
    "    # find the center of the robot from max_loc, top left point of convolution template\n",
    "    estimated_posx = max_loc [0] - math.floor ( true_robot_length * math.cos ( pos [2] ) )\n",
    "    estimated_posy = max_loc [1] + capt_ydim + math.floor ( true_robot_length * math.sin ( pos [2] ) )\n",
    "    cv2.rectangle(true_map, (estimated_posx , estimated_posy), (estimated_posx +10, estimated_posy+10), 255,2)\n",
    "\n",
    "    return np.array ( [estimated_posx , estimated_posy] )\n",
    "##################################################################################################\n",
    "#Image processing functions___END\n",
    "##################################################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load img from computer\n",
    "#img = prepare_image(img_path)#We load the image, we rotate it if needed to have a horizontal image.\n",
    "#or get it from the webcam\n",
    "#cap = cv2.VideoCapture(1)#put 1 if an external webcam is used\n",
    "#cap.set(3,1200)#width\n",
    "#cap.set(4,1000)#height\n",
    "#success, img = cap.read()\n",
    "#display(img)\n",
    "\n",
    "\n",
    "#true_map = get_clean_map(img)\n",
    "#for_global_search = cv2.resize(true_map, (100,50))#apply resizing to reduce computation cost of global search\n",
    "#start = robot_detection(for_global_search)#starting point for global search\n",
    "#end = goal_detection(for_global_search)#ending point for global search\n",
    "#map_global_search = get_global_search_map(for_global_search)\n",
    "#map_bin = map_global_search\n",
    "#print('start :',start)\n",
    "#print('goal :', end)\n",
    "#display(for_global_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Approaches\n",
    "Different approaches exist to find an optimal global path. In the beginning a visibility graph was considered. To find the shortest path from start to goal Dijkstra’s algorithm was used. Due to difficulties in detecting the corners of the objects a new approach had to be found. \n",
    "Therefore a cell decomposition/grid based approach was implemented.\n",
    "\n",
    "### A* on grid array\n",
    "To find the optimal path from the start to end position, the A* algorithm is used on a binary grid array. The A* algorithm can be seen as an extension of Dijkstra's algorithm. The grid array represents free spaces with a zero and obstacles (which the robot has to navigate around) with a one. For the implementation, a modified version of the code from the Exercise session 5 of the course MICRO-452  “Basics of Mobile Robotics” was used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Dijkstra - grid array\n",
    "Following function allow the testing of the Dijkstra's allgorithm on a binary grid array, without needing all the setup (robot and surrounding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "\n",
    "# Adding the src folder in the current directory as it contains the necessary scripts\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "import PathPlanning\n",
    "\n",
    "#max_val_column = 200 # Size of the map\n",
    "#max_val_row=100\n",
    "#occupancy_grid, cmap=PathPlanning.create_random_occupancy_grid(max_val_column, max_val_row)\n",
    "#print(occupancy_grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define start and end goal -> values have to be changed, that the are on empty spot and that a path between them exists\n",
    "#test_start=(0,0)#value has to be changed\n",
    "#test_goal=(99,199)#value has to be changed\n",
    "#path=PathPlanning.generate_global_path(test_start, test_goal, occupancy_grid)\n",
    "#path=np.fliplr(path)\n",
    "#print(f\"path (form start to goal)\")\n",
    "#print(f\"x={path[1]}\")\n",
    "#print(f\"y={path[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control\n",
    "\n",
    "Once the path planning has been done, we need to get the Thymio robot to follow the planned path.\n",
    "We do so by implementing a pilot, which uses knowledge of the robot's location relative to its next goal to calculate the motor inputs necessary to reach this goal. \n",
    "\n",
    "Unfortunately, the robot does not always have perfect knowledge of its position at all times, so it must be estimated from the limited measurements it does have access to, namely\n",
    "- motor odometry\n",
    "- ground IR sensors \n",
    "- horizontal proximity sensors\n",
    "\n",
    "Because both the movement process and these measurements are quite noisy, simply dead reckoning the robot's position is not accurate enough. We need to implement a stochasitc state estimator to try to estimate the robot's position from the noisy measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img src=\"./imgs/control-Page-1.png\" />\n",
    "\n",
    "> *Figure 1: Control diagram of the robot and pilot*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator\n",
    "A number of stochastic estimation methods exist. The approach chosen in this project, the Kalman Filter, is computationally one of the simplest. It was chosen in the hopes of making the system more reactive. The Kalman filter is normally used for systems with linear behavior, but it can relatively easily be extended to non-linear systems that do not vary too abruptly:\n",
    "$$ \\begin{eqnarray}\n",
    "\\bar{\\mathbf x} &=& g(\\hat{\\mathbf x}, \\mathbf u)\\\\\n",
    "\\bar{s} &=& G_{\\mathbf x,\\mathbf u}\\cdot s\\cdot G_{\\mathbf x,\\mathbf u}^T\\\\\n",
    "K       &=& \\bar{s}\\cdot C^T\\cdot (C\\cdot\\bar{s}\\cdot C^T + R)^{-1}\\\\\n",
    "\\hat{\\mathbf x} &=& \\bar{\\mathbf x} + K(\\mathbf y-C\\bar{\\mathbf x})\\\\\n",
    "s       &=& (I - K C)\\cdot \\bar{s}\n",
    "\\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Signal\n",
    "The Thymio robot may be controlled throught its two motors. Our control signal $\\mathbf u$ will therefore be : \n",
    "$$ \\mathbf u = \\begin{bmatrix} u_r\\\\ u_l \\end{bmatrix}$$\n",
    "\n",
    "## State \n",
    "The state of the robot $\\mathbf x$ is defined as follows :\n",
    "$$ \\mathbf{x} = \\begin{bmatrix}x\\\\ y \\\\ \\phi \\\\ v_r \\\\ v_l\\end{bmatrix}$$\n",
    "where $[x , y]^T$ are the robot's position in space, $\\phi$ is its angular orientation with respect to the x-axis, and $[v_r , v_l]^T$ are the speed on the right and left wheels, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Model\n",
    "To apply the extended Kalman Filter, we need to define a motion model $\\mathbf x^{+} = g(\\mathbf{x,u})$. \n",
    "The simplest motion model for a robot such as the Thymio can be calculated in two steps. First, the intermediary values describing the change in angle and in distance are calculated:  \n",
    "$$ \\begin{bmatrix}dl\\\\ d\\phi\\end{bmatrix} = B\\cdot \\mathbf u$$ \n",
    "\n",
    "for $B=c \\cdot T_s\\begin{pmatrix} 0.5 & 0.5 \\\\  \\frac{1}{d} & -\\frac{1}{d} \\end{pmatrix}$, where $T_s$ is the sample time and $d$ is the distance between the wheels. $c$ is a conversion constant to convert between Thymio speed and $mm/s$. \n",
    "Then the actual states may then be approximated as:\n",
    "$$\\begin{eqnarray}\n",
    "\\phi^{+} &=& \\phi + d\\phi\\\\\n",
    "x^{+}    &=& x + dl \\cdot cos(\\phi^{+})\\\\\n",
    "y^{+}    &=& y + dl \\cdot sin(\\phi^{+})\\\\\n",
    "v_r^{+}  &=& u_r\\\\\n",
    "v_l^{+}  &=& u_l\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "In order to linearize the motion model for use in the Kalman filter, we must calculate its Jacobian: $$G_{\\mathbf{x,u}} = \\begin{bmatrix} \n",
    "\\frac{\\partial g}{\\partial x}(\\mathbf{x,u}) &\n",
    "\\frac{\\partial g}{\\partial y}(\\mathbf{x,u}) & \n",
    "\\frac{\\partial g}{\\partial \\phi}(\\mathbf{x,u}) & \n",
    "\\frac{\\partial g}{\\partial v_r}(\\mathbf{x,u}) & \n",
    "\\frac{\\partial g}{\\partial v_l}(\\mathbf{x,u}) \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "This can be done analytically, but it is too tedious for this report. The result can be found in the code in the file Robot.py.\n",
    "\n",
    "Finally, we must estimate the uncertainty added by an update step of the motion process. We can measure the uncertainty on the motion process by wheel, which gives us $\\mathbf{q} = \\begin{bmatrix} q_r \\\\ q_l \\end{bmatrix}$ resulting in a coviariance matrix $Q_{\\mathbf u}$ with diagonal $\\mathbf{q}$.\n",
    "We can then apply the propagation of covariance formula though the linearized system (the Jacobian $G_{\\mathbf{x,u}}$) in order to estimate the uncertainty on the robot state gained due to noise for a single process step:\n",
    "$$ Q = G_{\\mathbf{x,u}} \\begin{pmatrix} \\mathbf 0 & 0 \\\\ 0& Q_{\\mathbf{u}}\\end{pmatrix} G_{\\mathbf {x,u}}^T$$\n",
    "**Note:** This is in fact illegal. The real probablity density of the error added to the robot's state is highly non-linear. It follows a banana-shape, as the anguar uncertainty is much larger than the distance uncertainty. However, it was estimate(d) that the robot would have ample chance to localize itself and thus reduce the uncertainty to a level where the non-linearity doesn't matter as much.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor interpretation \n",
    "\n",
    "As mentioned above, the robot has access to several sensors which give it information about the world surrounding it. \n",
    "In our application, the sensors available to us do not allow a reversible mapping into state-space (i.e: the sensor function $\\mathbf y = h(\\mathbf x)$ is non-invertible). This is because the map is made up of a repeating grid, so we have no information on the absolute localization, and furthermore from the pont of view of the sensor, there is no way to tell whether the detected line is horizontal or vertical.\n",
    "We must therefore do a preprocessing step on the raw sensor values to put them into a useable form for our (over)extended Kalman estimator.\n",
    "\n",
    "A first approach was, whenever a line was detected by a sensor to simply take the coordinate of the closest gridline to the estimated position of that sensor as the \"measured value\" $\\tilde{\\mathbf y}$. This approach however does not take into account the fact that the estimated postion of the robot can be quite uncertain, so the returned \"measured values\" are very often wrong.\n",
    "\n",
    "A different approach is to incorporate our estimated uncertainty into the sensor interpretation. Instead of looking for the closest gridline, we look for the most likely coordinates in which we could encounter a gridline, as shown in figure 3 below. These are then taken as $\\tilde{\\mathbf y}$.\n",
    "This approach was adapted from : https://www.cs.cmu.edu/~motionplanning/papers/sbp_papers/integrated2/machler_grid_odometry.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img src=\"./imgs/pdf-based_lacation_estimate.png\" alt=\"pdf-based_lacation_estimate\" style=\"width: 400px;\"/>\n",
    "\n",
    "> *Figure 2 : PDF-based location estimation. The real position of the sensor in space is unkown, so it is modelled as multivariate gaussian probability density function. When a gridline is encountered, we can calculate the position on the gridlines with the highest probablilty, and take this as our new position estimate.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import serial\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from threading import Timer\n",
    "from IPython import display as ds\n",
    "\n",
    "# Adding the src folder in the current directory as it contains the necessary scripts\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "from Thymio import Thymio\n",
    "import ReapeatedTimer\n",
    "import Field\n",
    "import Robot\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robot_thread(field, robot, goals, camera_pos):\n",
    "    \"\"\" do the robot control cycle \"\"\"\n",
    "\n",
    "    if not goals :\n",
    "            rt.stop()   #this is kind of bad, but i'm no python expert, so it'll have to do\n",
    "            return 0\n",
    "\n",
    "    u,local_obstacle = robot.pilot(goals)\n",
    "\n",
    "    if not local_obstacle :\n",
    "        robot.do_motion(u)\n",
    "\n",
    "    sensor_data = robot.do_measure()\n",
    "    \n",
    "    if local_obstacle : \n",
    "        u = sensor_data[9:11,:]\n",
    "        u[u>2**15] = u[u>2**15] - 2**16\n",
    "\n",
    "    y = robot.sensor_interpretation(sensor_data, field)\n",
    "\n",
    "    if camera_pos.x.size :                                    # if a camera position is available, update\n",
    "        c0 = np.concatenate((np.eye(3), np.zeros((3,2))), axis = 1)\n",
    "        y0 = camera_pos.x[0:3,:]\n",
    "        r0 = np.array([1, 1, .01])**2                           # we assume the camera position is the most precise possible\n",
    "        robot.R = np.diag(np.concatenate((r0, np.diag(robot.R)), axis=0))  \n",
    "        robot.C = np.concatenate((c0, robot.C))\n",
    "        y = np.concatenate((y0, y), axis=0)\n",
    "        camera_pos.x = np.array([])\n",
    "\n",
    "    xtemp, stemp = robot.kalman_estimator(u, y)\n",
    "\n",
    "    robot.xodo = robot.motion_model(robot.xodo, u)             # make a purely odometry-based estimate, for visual comparison\n",
    "\n",
    "    field.xhat  = np.concatenate((field.xhat, xtemp), axis=1)\n",
    "    field.xodo  = np.concatenate((field.xodo, robot.xodo), axis=1)\n",
    "    field.s.append(2*np.sqrt(stemp[0,0]+stemp[1,1]))  \n",
    "        \n",
    "        \n",
    "def camera_thread(field, cap, camera_pos) : \n",
    "    \"\"\" do the camera position acquisition cycle \"\"\"\n",
    "    \n",
    "    success, img = cap.read()                                  # read the image from the  fixed camera\n",
    "    true_map     = get_clean_map(img)                          # get the clean in good resolution\n",
    "    \n",
    "    x = util.adapt_vision_coords(robot_detection(true_map), field)# detect the position of the robot on the map\n",
    "    \n",
    "    field.xreal = np.concatenate((field.xreal, x), axis=1)\n",
    "    camera_pos.x = x.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Avoidance Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot      = Robot.NewRobot()                                                   # robot \n",
    "field      = Field.NewField(1200, 800, 200, 4)                                  # table\n",
    "\n",
    "robot.th   = Thymio.serial(port=\"COM15\", refreshing_rate=0.1)                   # set up Thymio communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setup vision\n",
    "cap_size          = (1200, 1000)\n",
    "global_shape      = (90,  60)\n",
    "cap               = cv2.VideoCapture(1)                                         # put 1 if an external webcam is used\n",
    "cap.set(3,cap_size[0])                                                          # width\n",
    "cap.set(4,cap_size[1])                                                          # height\n",
    "success, img      = cap.read()\n",
    "plt.imshow(img)\n",
    "#display(img)\n",
    "\n",
    "true_map          = get_clean_map(img)\n",
    "\n",
    "start             = util.rescale_int_coords(robot_detection(true_map), (true_map.shape[1], true_map.shape[0]), global_shape)  # starting point for global search\n",
    "end               = util.rescale_int_coords(goal_detection(true_map),  (true_map.shape[1], true_map.shape[0]), global_shape)  # end point for global search\n",
    "map_global_search = get_global_search_map(cv2.resize(true_map, global_shape))# apply resizing to reduce computation cost of global search\n",
    "\n",
    "# setup global path finding\n",
    "occupancy_grid    = np.flipud(np.asarray(map_global_search))                     # make occupancy grid and change coordinate system           \n",
    "#occupancy_grid, _ = create_random_occupancy_grid(90,60)\n",
    "\n",
    "#path              = generate_global_path((occupancy_grid.shape[0]-start[1], start[0]), (occupancy_grid.shape[0]-end[1], end[0]), occupancy_grid)\n",
    "path              = generate_global_path((start[1], start[0]), (end[1], end[0]), occupancy_grid)\n",
    "path              = util.adapt_pathfinder_coords(path, field, occupancy_grid.shape)\n",
    "\n",
    "\n",
    "\n",
    "# setup robot \n",
    "Tr = .2                                                                          # robot update timestep\n",
    "Tc =  1                                                                          # camera update timestep\n",
    "\n",
    "startpos    = util.adapt_vision_coords(robot_detection(true_map), field)         # starting point for pilot\n",
    "robot.xhat  = startpos.copy()                                                    # setup robot\n",
    "robot.xodo  = startpos.copy()\n",
    "robot.s     = np.diag([1, 1, .1, 20, 20])**2\n",
    "\n",
    "robot.d     = 90\n",
    "robot.set_Ts(Tr)\n",
    "robot.maxv  = 10\n",
    "robot.maxw  = np.pi/32\n",
    "\n",
    "field.goals = path\n",
    "field.xreal = startpos.copy()                                                    # prepare record of (mis)deeds\n",
    "field.xhat  = robot.xhat.copy()\n",
    "field.xodo  = robot.xodo.copy()\n",
    "field.s     = [2*np.sqrt(robot.s[0,0]+robot.s[1,1])]\n",
    "\n",
    "goals       = np.ndarray.tolist(field.goals)\n",
    "\n",
    "class Camera_Pos :\n",
    "    def __init__ (self): \n",
    "        pass\n",
    "camera_pos = Camera_Pos()\n",
    "camera_pos.x = np.array([])                                                      # communication variable\n",
    "\n",
    "rt = RepeatedTimer.Thread(Tr, robot_thread,  field, robot, goals, camera_pos);   # start threads\n",
    "ct = RepeatedTimer.Thread(Tc, camera_thread, field, cap, camera_pos);\n",
    "\n",
    "try:\n",
    "    #print(\"Starting Run\")  \n",
    "    for i in range(20):\n",
    "        time.sleep(1) \n",
    "        plt.clf();\n",
    "        plt.gcf().set_size_inches(12,8)\n",
    "        plt.imshow(occupancy_grid,cmap=colors.ListedColormap(['white', 'black']), origin = 'lower', extent=(field.xmin, field.xmax, field.ymin, field.ymax))\n",
    "        field.plot()\n",
    "        ds.display(plt.gcf())\n",
    "        ds.clear_output(wait=True)\n",
    "        \n",
    "        if not goals : \n",
    "            print(\"arrived\")\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    rt.stop()\n",
    "    ct.stop()\n",
    "    robot.th.set_var(\"event.args\", 2)\n",
    "    robot.do_motion(np.zeros((2,1)))\n",
    "    print(\"Done\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.do_motion(np.zeros((2,1)))\n",
    "robot.th.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(occupancy_grid,cmap=colors.ListedColormap(['white', 'black']), origin = 'lower', extent=(field.xmin, field.xmax, field.ymin, field.ymax))\n",
    "field.plot()\n",
    "plt.gcf().set_size_inches(12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
