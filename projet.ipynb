{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group 3**: Loïk Vuilleumier (283675), Marc Favier (319571), Michael Biselx (283812), Samuel Bumann (284704)\n",
    "\n",
    "---\n",
    "# Basics of Mobile Robotics Miniproject\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Basics-of-Mobile-Robotics-Miniproject\" data-toc-modified-id=\"Basics-of-Mobile-Robotics-Miniproject-1\">Basics of Mobile Robotics Miniproject</a></span></li><li><span><a href=\"#Introduction:\" data-toc-modified-id=\"Introduction:-2\">Introduction:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Environment\" data-toc-modified-id=\"Environment-2.1\">Environment</a></span></li></ul></li><li><span><a href=\"#Image-Processing\" data-toc-modified-id=\"Image-Processing-3\">Image Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Detecting-corner-of-the-robot's-environment\" data-toc-modified-id=\"Detecting-corner-of-the-robot's-environment-3.1\">Detecting corner of the robot's environment</a></span></li><li><span><a href=\"#The-image-processing-can-be-divided-into-four-steps-:\" data-toc-modified-id=\"The-image-processing-can-be-divided-into-four-steps-:-3.2\">The image processing can be divided into four steps :</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Obtain-an-image-of-the-region-of-interest-:-the-map-of-the-environment.\" data-toc-modified-id=\"Step-1:-Obtain-an-image-of-the-region-of-interest-:-the-map-of-the-environment.-3.2.1\">Step 1: Obtain an image of the region of interest : the map of the environment.</a></span></li><li><span><a href=\"#Step-2:-Find-the-position-of-the-robot.\" data-toc-modified-id=\"Step-2:-Find-the-position-of-the-robot.-3.2.2\">Step 2: Find the position of the robot.</a></span></li><li><span><a href=\"#Step-3:-Find-the-goal.\" data-toc-modified-id=\"Step-3:-Find-the-goal.-3.2.3\">Step 3: Find the goal.</a></span></li><li><span><a href=\"#Step-4:-Get-a-map-for-the-global-path-planning.\" data-toc-modified-id=\"Step-4:-Get-a-map-for-the-global-path-planning.-3.2.4\">Step 4: Get a map for the global path planning.</a></span></li></ul></li><li><span><a href=\"#Testing-the-Module\" data-toc-modified-id=\"Testing-the-Module-3.3\">Testing the Module</a></span></li></ul></li><li><span><a href=\"#Global-Path-Planning\" data-toc-modified-id=\"Global-Path-Planning-4\">Global Path Planning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Different-Approaches\" data-toc-modified-id=\"Different-Approaches-4.1\">Different Approaches</a></span><ul class=\"toc-item\"><li><span><a href=\"#A*-on-grid-array\" data-toc-modified-id=\"A*-on-grid-array-4.1.1\">A* on grid array</a></span></li><li><span><a href=\"#A*-algorithm-theory\" data-toc-modified-id=\"A*-algorithm-theory-4.1.2\">A* algorithm theory</a></span></li></ul></li><li><span><a href=\"#Testing-the-A*-Implementation\" data-toc-modified-id=\"Testing-the-A*-Implementation-4.2\">Testing the A* Implementation</a></span></li></ul></li><li><span><a href=\"#Localistion-and-Control\" data-toc-modified-id=\"Localistion-and-Control-5\">Localistion and Control</a></span><ul class=\"toc-item\"><li><span><a href=\"#Theory\" data-toc-modified-id=\"Theory-5.1\">Theory</a></span><ul class=\"toc-item\"><li><span><a href=\"#Estimator\" data-toc-modified-id=\"Estimator-5.1.1\">Estimator</a></span></li><li><span><a href=\"#Control-Signal\" data-toc-modified-id=\"Control-Signal-5.1.2\">Control Signal</a></span></li><li><span><a href=\"#State\" data-toc-modified-id=\"State-5.1.3\">State</a></span></li><li><span><a href=\"#Motion-Model\" data-toc-modified-id=\"Motion-Model-5.1.4\">Motion Model</a></span></li><li><span><a href=\"#Sensor-interpretation\" data-toc-modified-id=\"Sensor-interpretation-5.1.5\">Sensor interpretation</a></span></li></ul></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-5.2\">Setup</a></span></li></ul></li><li><span><a href=\"#Local-Navigation\" data-toc-modified-id=\"Local-Navigation-6\">Local Navigation</a></span></li><li><span><a href=\"#Final-Program\" data-toc-modified-id=\"Final-Program-7\">Final Program</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-8\">Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Normal-Run\" data-toc-modified-id=\"Normal-Run-8.1\">Normal Run</a></span></li><li><span><a href=\"#No-Vision\" data-toc-modified-id=\"No-Vision-8.2\">No Vision</a></span></li><li><span><a href=\"#Kidnapping\" data-toc-modified-id=\"Kidnapping-8.3\">Kidnapping</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-9\">Conclusion</a></span><ul class=\"toc-item\"><li><span><a href=\"#Image-Processing\" data-toc-modified-id=\"Image-Processing-9.1\">Image Processing</a></span></li><li><span><a href=\"#Global-Path-Planning\" data-toc-modified-id=\"Global-Path-Planning-9.2\">Global Path Planning</a></span></li><li><span><a href=\"#Localisation-and-Control\" data-toc-modified-id=\"Localisation-and-Control-9.3\">Localisation and Control</a></span></li><li><span><a href=\"#Local-Navigation\" data-toc-modified-id=\"Local-Navigation-9.4\">Local Navigation</a></span></li><li><span><a href=\"#Overall\" data-toc-modified-id=\"Overall-9.5\">Overall</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "The objective of this project is to create a program in Python/Aseba that:\n",
    "- Uses the visual feedback of a webcam to detect the starting position of a robot, its goal position and 2D obstacles in a given environment with some known specifications described in the following section Environment.\n",
    "- Finds the optimal path from the start position of the robot to its goal between the globals obstacles. It uses A* algorithm.\n",
    "- Controls the robot to follow the optimal path, by successively applying filters to estimate its position. The entries are the absolute position of the robot given by the camera, the odometry and proximity and black and white sensors underneath the robot.\n",
    "- Navigates around 3D local obstacles using local avoidance.\n",
    "\n",
    "## Environment\n",
    "The environment consists of a white table with a thin grid pattern (each square is 200mm x 200mm). The table has blue disks in all corners. The Thymio robot is located within the rectangle spanned by the blue circles, along with a green disk that represents the goal position and black shapes which represent the obstacles. Two red disks of different diameters are located on top of the Thymio. The global obstacles are in 2D because 3D obstacles with an non vertical view (and even whith such a view) changes a lot in size. For local avoidance we use white 3D obstacles, that are not seen by the vision module.\n",
    "\n",
    "> <img src=\"./imgs/environment_titel.jpg\"  style=\"width: 400px;\"/>\n",
    "\n",
    "> *Figure 1: This figure shows the environment: the table with its grid pattern and the blue circles in the corners, the green goal circle, the black obstacle shapes and the Thymio robot with two red circles.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing\n",
    "## Detecting corner of the robot's environment\n",
    "We want to get rid off all the random components introduced by taking a picture at a skewed angle. This includes translations, rotations, scaling and most importantly perspective transformations. Therefore we want to search for four points, which we willingly place at known distances and angles from each other, to standardize our image. \n",
    "\n",
    "> <img src=\"./imgs/original_map.png\" alt=\"original_map\" style=\"width: 400px;\"/>\n",
    "\n",
    "> *Figure 2: Here is an example of the image we are working with.*\n",
    "\n",
    "## The image processing can be divided into four steps :\n",
    "\n",
    "### Step 1: Obtain an image of the region of interest : the map of the environment.\n",
    "The best is to detect the four extreme angles of the rectangle environment. It's increases the precision of the transformation by decreasing the relative error on their distance to each other. There are several ways to find these corners : \n",
    "\n",
    "- detect the lines of the rectangle (hough transform or edge detection) but there will be too many lines with obstacles. It could have been possible to detect the edges of the outer rectangle defining the map, fill the obtained rectangle (meaning put all the pixels inside its closed borders to one), next erode the image to delete all exterior components to the rectangle, our region of interest. Then we just have to dilate to have only the rectangle with its original size on a black background, which is very easy to detect. But our background has too many long lines throughout the whole image (for example legs of the table) then filling closed borders would have lead to filling most of the image. To summarize, this is not a good method because our region of interest does not produce the biggest closed border, it's included in other closed borders.\n",
    "- convolve the image with a corner (two small lines 90 degrees to each others) has the advantage of being quite scale invariant but depends on rotation. The results were very bad as the image often presents rotations. Furthermore with the image with grid pattern we can't distinguish all the corners.\n",
    "- convolve with a disk has the advantage of being rotation invariant, which is better than scale invariant. So we introduce specific patterns at the corners to indirectly detect them. Its better to have disk with specific color, but even then convolution was not good enough (with perspective, disk become ellipsoids and do not fit the pattern anymore).\n",
    "- use the HSV (Hue Saturation Value) representation of the image to easily detect colors. We pass from RGB representation where it's hard to detect specific colors to the HSV where saturated colors (meaning very red, very blue or very green) are easy to detect on a neutral backgound. Therefore we just need colors, without specific shape. We kept the disks because they are more robust to bad detection : when some pixels are not detected they mostly stay as one connected region of pixels with the same centroid.\n",
    "We chose the last option, the most efficient.\n",
    "\n",
    "Therefore four blue disks are placed next to the four corners, we filter those blue colors of the image and we obtain a mask which is one for each of those blue pixels and zero four the others. We apply an erosion followed by a dilation of the same size to remove all the possibles noisy isolated pixels which could have been considered as blue. In fact if our selection of pixels is too restrictive we might not detect the disks and if it's too large we might detect unexpected regions as blue. Next, we apply a labeling of connected components. Meaning that we associate a unique value to each region of pixels which are linked to each others. We ideally want five labels : one for the background and one for each of the four corners. But even with erosion, there might be some unexpected one values in the mask. Therefore we sort all the labels with respect to their number of pixels and we take the four biggest. After that we compute the centroides of those four regions. We put also all the blue pixels to white in order to avoid to detect them again in the others functions. Finally we apply the opencv function \"cv2.getPerspectiveTransform\" to fit the detected corners to defined values : the corners of an image of size (1200, 800). We obtain the following image :\n",
    "\n",
    "> <img src=\"./imgs/cropped_map.png\" alt=\"cropped_map\" style=\"width: 400px;\"/>\n",
    "\n",
    "> *Figure 3: This figure shows a map of the robot's environment after transformation.*\n",
    "\n",
    "### Step 2: Find the position of the robot.\n",
    "We apply a similar operation to find the robot. We go through the HSV representation of the obtained map and filter the red components. We sort the labels by the number of elements and from here the function is different. We take the centroid of the biggest label, in terms of number of pixels, as the robot position in (x,y) coordinate and we use the centroid of the second biggest label, again in terms of number of pixels, to compute the robot's angle.\n",
    "\n",
    "Finally, a heuristic correction on the position is applied, to compensate for the effect of the height of the robot and the camera's field of view.\n",
    "\n",
    "### Step 3: Find the goal.\n",
    "We again go through the HSV representation, filter green components and compute the centroid of the goal.\n",
    "\n",
    "### Step 4: Get a map for the global path planning.\n",
    "Here we go from RGB to grayscale representation of the environment. We apply an erosion and dilation to get rid off the line used for local positioning. We binarize the image using a fixed value. Theorically Otsu method is supposed to work better as it maximizes interclass variance end minimising intraclass variance. But for our case, a fixed threshold was enough and even a bit better. We use this binary image to find the global path.\n",
    "\n",
    "> <img src=\"./imgs/map_bin.png\" alt=\"map_bin\" style=\"width: 400px;\"/>\n",
    "\n",
    "> *Figure 4: The binary map with dilated obstacles.*\n",
    "\n",
    "\n",
    "The steps 1 and 2 are repeated while the robot is moving. We don't really need to do each time the step 1 which recomputes the map but this ensures that even if the camera moves a bit the position of the robot computed in step 2 is still valid. It's a security step. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-image  # used inside the Vision module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell imports the necessary libraries and modules. The image processing module can be found under [Vision](src/Vision.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Adding the src folder in the current directory as it contains the Vision module\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "import Vision as vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_thread(cap, camera_pos) : \n",
    "    \"\"\" do the camera position acquisition cycle \"\"\"\n",
    "    \n",
    "    x = np.array([])\n",
    "    while not x.size :\n",
    "        success, img = cap.read()                                  # read the image from the fixed camera\n",
    "        if success :\n",
    "            success2, true_map     = vis.get_clean_map(img, [1200, 800])     # get the clean in good resolution\n",
    "            if success2 :\n",
    "                x            = vis.robot_detection(true_map)           # find the robot\n",
    "            else:\n",
    "                break\n",
    "        time.sleep(.1)\n",
    "    \n",
    "    camera_pos.x = x.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Module\n",
    "The following cell allows the testing of the setup, independently of other modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get img from the webcam\n",
    "# cap_size  = (1200, 1000)\n",
    "# map_shape = [1200,  800]\n",
    "# cap       = cv2.VideoCapture(2)                       # put 1 if an external webcam is used\n",
    "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, cap_size[0])        # width\n",
    "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT,cap_size[1])        # height\n",
    "# cap.set(cv2.CAP_PROP_AUTO_EXPOSURE,1)                 # set auto exposure\n",
    "\n",
    "# robot_pos = np.array([])\n",
    "\n",
    "# while True :\n",
    "#     success, img = cap.read()\n",
    "#     if success :\n",
    "#         success2, true_map = vis.get_clean_map(img, map_shape)                  # get a clean map\n",
    "#         if success2 :\n",
    "#             robot_pos = vis.robot_detection(true_map)                           # find the robot, annotate on map\n",
    "#             goal_pos  = vis.goal_detection(true_map)                            # find the goal, annotate on map\n",
    "#             if robot_pos.size :\n",
    "#                 obs_map = vis.get_global_search_map(true_map, robot_pos)        # find obstacles\n",
    "#                 cv2.imshow(\"Obstacles\", obs_map)\n",
    "\n",
    "#             true_map = vis.annotate_map(true_map, robot_pos, goal_pos)\n",
    "\n",
    "\n",
    "#         cv2.imshow(\"Map\", true_map)\n",
    "#         cv2.imshow(\"Raw\", img)                                    # display\n",
    "\n",
    "#     else:\n",
    "#         print(\"Couldn't capture image\")\n",
    "#         break\n",
    "\n",
    "#     if (cv2.waitKey(1) & 0xFF) == ord('q'):                       # quit when 'Q' key is pressed\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Path Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Approaches\n",
    "Different approaches exist to find an optimal global path. In the beginning, the use of a visibility graph was considered. To find the shortest path from start to goal, Dijkstra’s algorithm was applied to the visibility graph. The visibility graph is in fact a fast tool to find the optimal path. But it has some incompatibilities with the image processing part and only provides an advantage in terms of computing speed which is not a critical point, as we only search for the global path only once, at the beginning. It's very easy to produce an occupancy grid, it's a simple binarisation of the grayscale map. But finding corners of obstacles is very difficult and restricted to sharp shapes with acute angles. We could use the Harris corner detector which computes the derivative of the image in several direction but this method often doesn't give the expected number of corners by detecting corners in globally straight but locally sawtooth lines. Computing the convex hull of those points is a good way to reduce their number, but it drastically reduces the possible obstacle shapes to sharp and convex shapes. Therefore the visibility graph has many more drawbacks than advantages (especially given the additionnal computation in image processing might compensate the speed of the visibility graph). We prefered to change to another method.\n",
    "Therefore a cell decomposition/grid based approach was implemented.\n",
    "\n",
    "### A* on grid array\n",
    "To find the optimal path from the start to end position, the A* algorithm is used on a binary grid array (occupancy grid). The A* algorithm can be seen as an extension of Dijkstra's algorithm. The grid array represents free spaces with a zero and obstacles (which the robot has to navigate around) with a one. For the implementation, a modified version of the code from the Exercise session 5 of the course MICRO-452 “Basics of Mobile Robotics” was used. The code can be found in the [PathPlanning](src/PathPlanning.py) module.\n",
    "\n",
    "### A* algorithm theory\n",
    "The A* algorithm combines a motion cost with a heuristic cost. The motion cost and the heuristic function assign a cost to each cell of the array. The motion cost corresponds to the shortest number of steps from the start position to the given cell (obstacles are avoided). \n",
    "The heuristic cost implemented in this case corresponds to the distance between the goal position and the cell (obstacles are not avoided). The final cost for every cell is calculated by adding the corresponding motion and heuristic cost together. To find the optimal path one has to progress step by step to the next adjacent cell with the lowest final cost, starting from the start position until the goal is reached (see fig 5).\n",
    "Note that the motion and final cost are not calculated for every cell. The algorithm only calculates the cost for “promising” cells (see fig 6).  \n",
    "\n",
    "> <img src=\"./imgs/A_star_example.jpg\" />\n",
    "\n",
    "> *Figure 5: This figure shows the motion, heuristic and final cost implemented on an example array. The different costs for every cell are described by the corresponding number in every cell. Note that the red cell is the start, the green cell is the goal, the black cells are obstacles, white cells are free spaces and the gray cells describe the final path.*\n",
    "\n",
    "\n",
    "> <img src=\"./imgs/global_path_test.png\" />\n",
    "\n",
    "\n",
    "> *Figure 6: This figure shows the result of the A-star algorithm on a test array. Note, that the white cells are free spaces, the red cells are obstacles, the yellow cells are explored cells and that the optimal path is represented by the blue line.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "# Adding the src folder in the current directory as it contains the PathPlanning module\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "import PathPlanning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the A* Implementation\n",
    "Following cell allows the testing of the A* allgorithm on a binary grid array, without needing all the setup (robot and surrounding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_val_column = 100 # Size of the map\n",
    "# max_val_row = 50\n",
    "# occupancy_grid, cmap=PathPlanning.create_random_occupancy_grid(max_val_column, max_val_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Define start and end goal -> values have to be changed, that the are on empty spot and that a path between them exists\n",
    "# test_start=(0,0)    #value has to be changed\n",
    "# test_goal=(49,99)  #value has to be changed\n",
    "# path=PathPlanning.generate_global_path(test_start, test_goal, occupancy_grid)\n",
    "# path=np.fliplr(path)\n",
    "# print(f\"path (form start to goal)\")\n",
    "# print(f\"x={path[1]}\")\n",
    "# print(f\"y={path[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localistion and Control\n",
    "\n",
    "## Theory\n",
    "Once the path planning has been done, we need to get the Thymio robot to follow the planned path.\n",
    "We do so by implementing a pilot, which uses knowledge of the robot's location relative to its next target to calculate the motor inputs necessary to reach it. \n",
    "\n",
    "Unfortunately, the robot does not always have perfect knowledge of its position at all times, so it must be estimated from the limited measurements it does have access to, namely:\n",
    "- camera localisation\n",
    "- motor odometry\n",
    "- ground IR sensors \n",
    "- horizontal proximity sensors\n",
    "\n",
    "Because both the movement process and these measurements are quite noisy or only intermittant, simply dead reckoning the robot's position is not accurate enough. We need to implement a stochasitc state estimator to try to estimate the robot's position from the noisy measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img src=\"./imgs/control-Page-1.png\" />\n",
    "\n",
    "> *Figure 7: Control diagram of the robot and pilot*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator\n",
    "A number of stochastic estimation methods exist. The approach chosen in this project, the Kalman Filter, is computationally one of the simplest. It was chosen in the hopes of making the system more reactive. The Kalman filter is normally used for systems with linear behavior, but it can relatively easily be extended to non-linear systems that do not vary too abruptly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Signal\n",
    "The Thymio robot may be controlled through its two motors. Our control signal $\\mathbf u$ will therefore be : \n",
    "$$ \\mathbf u = \\begin{bmatrix} u_r\\\\ u_l \\end{bmatrix}$$\n",
    "\n",
    "### State \n",
    "The state of the robot $\\mathbf x$ is defined as follows :\n",
    "$$ \\mathbf{x} = \\begin{bmatrix}x\\\\ y \\\\ \\phi \\\\ v_r \\\\ v_l\\end{bmatrix}$$\n",
    "where $[x , y]^T$ are the robot's position in space, $\\phi$ is its angular orientation with respect to the x-axis, and $[v_r , v_l]^T$ are the speed on the right and left wheels, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motion Model\n",
    "To apply the extended Kalman Filter, we need to define a motion model $\\mathbf x^{+} = g(\\mathbf{x,u})$. \n",
    "The simplest motion model for a robot such as the Thymio can be calculated in two steps. First, the intermediary values describing the change in angle and in distance are calculated:  \n",
    "$$ \\begin{bmatrix}dl\\\\ d\\phi\\end{bmatrix} = B\\cdot \\mathbf u , \\text{ for  } B=c \\cdot T_s\\begin{pmatrix} 0.5 & 0.5 \\\\  \\frac{1}{d} & -\\frac{1}{d} \\end{pmatrix} $$ \n",
    "\n",
    "where $T_s$ is the sample time and $d$ is the distance between the wheels. $c$ is a conversion constant to convert between Thymio speed and $mm/s$. \n",
    "Then the actual states may then be approximated as:\n",
    "$$\\begin{eqnarray}\n",
    "\\phi^{+} &=& \\phi + d\\phi\\\\\n",
    "x^{+}    &=& x + dl \\cdot cos(\\phi^{+})\\\\\n",
    "y^{+}    &=& y + dl \\cdot sin(\\phi^{+})\\\\\n",
    "v_r^{+}  &=& u_r\\\\\n",
    "v_l^{+}  &=& u_l\n",
    "\\end{eqnarray}$$\n",
    "Other motion models were studied, and can be found in the module code.\n",
    "\n",
    "In order to linearize the motion model for use in the Kalman filter, we must calculate its Jacobian. This can be done analytically, but it is too tedious for this report. The result can be found in the code for this module. \n",
    "$$G_{\\mathbf{x,u}} = \\begin{bmatrix} \n",
    "\\frac{\\partial g}{\\partial x}(\\mathbf{x,u}) &\n",
    "\\frac{\\partial g}{\\partial y}(\\mathbf{x,u}) & \n",
    "\\frac{\\partial g}{\\partial \\phi}(\\mathbf{x,u}) & \n",
    "\\frac{\\partial g}{\\partial v_r}(\\mathbf{x,u}) & \n",
    "\\frac{\\partial g}{\\partial v_l}(\\mathbf{x,u}) \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Finally, we must estimate the uncertainty added by an update step of the motion process. We can measure the uncertainty on the motion process for each motor individually, which gives us $\\mathbf{q} = \\begin{bmatrix} q_r \\\\ q_l \\end{bmatrix}$ resulting in a coviariance matrix $Q_{\\mathbf u}$ with diagonal $\\mathbf{q}$.\n",
    "We can then apply the propagation of covariance formula through the linearized system (the Jacobian $G_{\\mathbf{x,u}}$) in order to estimate the uncertainty on the robot state gained due to noise for a single process step:\n",
    "$$ Q = G_{\\mathbf{x,u}} \\begin{pmatrix} \\mathbf 0 & 0 \\\\ 0& Q_{\\mathbf{u}}\\end{pmatrix} G_{\\mathbf {x,u}}^T$$\n",
    "**Note:** This is in fact illegal. The real probablity density of the error added to the robot's state is highly non-linear. It follows a banana-shape, as the angular uncertainty is much larger than the distance uncertainty. However, it was estimated that the robot would have ample chance to localize itself and thus reduce the uncertainty to a level where the non-linearity doesn't matter as much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor interpretation \n",
    "\n",
    "As mentioned above, the robot has access to several sensors which give it information about the world surrounding it. \n",
    "In our application, most of the sensors available to us do not allow a reversible mapping into state-space (i.e: the sensor function $\\mathbf y = h(\\mathbf x)$ is non-invertible). This is because the map is made up of a repeating grid, so we have no information on the absolute localization (for the IR-sensors). Furthermore, from the point of view of the sensor, there is no way to tell whether the detected line is horizontal or vertical.\n",
    "We must therefore do a preprocessing step on the raw sensor values to put them into a useable form for our (over)extended Kalman estimator.\n",
    "\n",
    "A first approach was, whenever a line was detected by a sensor to simply take the coordinate of the closest gridline to the estimated position of that sensor as the \"measured value\" $\\tilde{\\mathbf y}$. This however does not take into account the fact that the estimated postion of the robot can be quite uncertain, so the returned \"measured values\" were very often wrong.\n",
    "\n",
    "A different approach is to incorporate our estimated uncertainty into the sensor interpretation. Instead of looking for the closest gridline, we look for the most likely coordinates in which we could encounter a gridline, as shown in figure 3 below. These are then taken as $\\tilde{\\mathbf y}$.\n",
    "This approach was adapted from the paper : https://www.cs.cmu.edu/~motionplanning/papers/sbp_papers/integrated2/machler_grid_odometry.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <img src=\"./imgs/pdf-based_lacation_estimate.png\" alt=\"pdf-based_lacation_estimate\" style=\"width: 200px;\"/>\n",
    "\n",
    "> *Figure 8 : Probability density-based location estimation. The real position of the sensor in space is unkown, so it is modelled as multivariate gaussian probability density function. When a gridline is encountered, we can calculate the position on the gridlines with the highest probablilty, and take this as our new position estimate.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The code for this module can be found under [Robot.py](src/Robot.py), and some generic-use code can also be found in [util.py](src/util.py) and [Field.py](src/Field.py).\n",
    "The code for creating [threads](src/RepeatedTimer.py) was adapted from code presented in the course MICRO-452 “Basics of Mobile Robotics” \n",
    "\n",
    "Below, we define the thread which controls the robot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import serial\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from IPython import display as ds\n",
    "\n",
    "# Adding the src folder in the current directory as it contains the necessary scripts\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "from Thymio import Thymio\n",
    "import RepeatedTimer\n",
    "import Field\n",
    "import Robot\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robot_thread(field, robot, goals, camera_pos):\n",
    "    \"\"\" do the robot control cycle \"\"\"\n",
    "\n",
    "    if not goals :\n",
    "            raise Exception(\"No more goals\")   #this is bad, but i'm no python expert, so it'll have to do\n",
    "            return \n",
    "\n",
    "    u,local_obstacle = robot.pilot(goals)\n",
    "\n",
    "    if not local_obstacle :\n",
    "        robot.do_motion(u)\n",
    "\n",
    "    sensor_data = robot.do_measure()\n",
    "    \n",
    "    if local_obstacle : \n",
    "        u = sensor_data[9:11,:]\n",
    "        u[u>2**15] = u[u>2**15] - 2**16\n",
    "\n",
    "    y = robot.sensor_interpretation(sensor_data, field)\n",
    "\n",
    "    if camera_pos.x.size :                                    # if a camera position is available, update\n",
    "        c0 = np.concatenate((np.eye(3), np.zeros((3,2))), axis = 1)\n",
    "        y0 = camera_pos.x[0:3,:]\n",
    "        r0 = np.array([1, 1, .1])**2                      # we assume the camera position is the most precise \n",
    "        robot.R = np.diag(np.concatenate((r0, np.diag(robot.R)), axis=0)) # possible to make our lives simpler \n",
    "        robot.C = np.concatenate((c0, robot.C))\n",
    "        y       = np.concatenate((y0, y), axis=0)\n",
    "        field.xreal  = np.concatenate((field.xreal, camera_pos.x), axis=1)\n",
    "        camera_pos.x = np.array([])\n",
    "\n",
    "    xtemp, stemp = robot.kalman_estimator(u, y)\n",
    "\n",
    "    robot.xodo = robot.motion_model(robot.xodo, u)             # make a purely odometry-based estimate, for comparison\n",
    "\n",
    "    field.xhat  = np.concatenate((field.xhat, xtemp), axis=1)\n",
    "    field.xodo  = np.concatenate((field.xodo, robot.xodo), axis=1)\n",
    "    field.s.append(2*np.sqrt(stemp[0,0]+stemp[1,1]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local navigation is programmed with Aseba and thus runs on the Thymio itself. This allows a quick reaction to local obstacles, because the data does not travel to the computer and back.\n",
    "\n",
    "The goal of the local navigation is to follow alongside the obstacle until the robot is back onto its global path.\n",
    "\n",
    "To follow the obstacle, mainly the outer infrared (IR) sensors of the Thymio are used, because they are the only sensors that return non-zero values when the Thymio is parallel to the obstacle. Furthermore, only the speed of the wheel which is further away from the obstacle is usually changed. The speed of the wheel which is closer to the obstacle is mostly constant. An exception is made when at least one of the three middle IR sensors returns a non-zero value, which means that the robot is not parallel to the obstacle anymore, but rather perpendicular. In this case the speed of both wheels are reduced, to avoid a collision with the obstacle.\n",
    "\n",
    "The speed of the wheel which is further away from the obstacle is controlled by a simple proportional controller. This means that the speed of this wheel is proportional to the error (difference between the desired sensor value and the real sensor value). \n",
    "\n",
    "In Aseba, a timer emits an event every 200ms. At each of these events, the speed of the wheel closer to the wall is updated, in order to avoid the obstacles. To determine the speeds of the two wheels, first it is determined if the obstacle is located on the left or right side of the robot. Then the speeds are calculated based on the sensor values and the proportional gain. Finally the speed target values of the Thymio are updated with the calculated speeds.\n",
    "\n",
    "The Python and the Aseba programs have to communicate with each other, to prevent that both programs interfere with each other by giving contradictory commands. For this purpose the <code>event.args</code> variables, which are available for both programs, were used. The variable <code>event.args[0]</code> describes if the robot is in global or local navigation. This is considered by both Python and Aseba programs, such that only one program gives commands to the robot.\n",
    "\n",
    "To switch from global to local navigation, <code>event.args[0]</code> is changed in Aseba, if a local obstacle is detected with the IR sensors. Furthermore, the flag <code>event.args[0]</code> determines if the Python or the Aseba program controls the robot.\n",
    "\n",
    "It is a bit harder to switch from global to local navigation. As seen in the course there are multiple solutions to this problem. The chosen solution consists of exiting local navigation if the robot gets back to the global path, as mentioned above. \n",
    "Since the global path is described by close points, the robot can be considered to be on the global path, if it is next to such a point. In this case the robot can continue with the global navigation, but it has to ignore the IR sensors for a moment to avoid getting back into local navigation instantly. However, then the possibility exists that the robot collides with the obstacle while ignoring the IR sensors. The solution implemented to solve this issue, is to assure that the end goal and the obstacle are on opposite sides of the robot. This way the robot can exit the local navigation without hitting the obstacle. \n",
    "\n",
    "A challenge encountered during the implementation of the local navigation was the robot turning early around the corner and bumping into it with its rear part. This is because the IR sensors are directed forward and that one wheel has a constant speed. The solution implemented to solve this problem is to delay the turning, when the obstacle is not detected by the sensors anymore. This is not an ideal solution but it is functional.\n",
    "\n",
    "The Aseba code can be found [here](Aseba/local_navigation.aesl), and the Python code is integrated in the [Robot](src/Robot.py) module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Program\n",
    "\n",
    "In order to run the final program, all the cells above must be run first. This is to ensure all the necessary functions and libraries have been declared and imported. Furthermore, the Aseba program must be loaded onto the Thymio robot, then the Aseba Studios must be closed.\n",
    "\n",
    "Once this has been done, the next cell must be run **once**, in order to set up the objects for the next run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camera_Pos :\n",
    "    def __init__ (self): \n",
    "        pass\n",
    "camera_pos = Camera_Pos()\n",
    "robot      = Robot.NewRobot()                                                   # robot \n",
    "field      = Field.NewField(1200, 800, 200, 4)                                  # table\n",
    "\n",
    "robot.th   = Thymio.serial(port=\"COM15\", refreshing_rate=0.1) # set up Thymio communication\n",
    "\n",
    "cap_size          = (1200, 1000)\n",
    "global_shape      = (90,  60)\n",
    "cap               = cv2.VideoCapture(2)                                         # put 1 if an external webcam is used\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, cap_size[0])                                  # width\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT,cap_size[1])                                  # height\n",
    "cap.set(cv2.CAP_PROP_AUTO_EXPOSURE,1)                                           # set auto exposure\n",
    "time.sleep(1)                                                                   # give the camera time to adjust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the following cell runs a single attempt to reach the goal. \n",
    "\n",
    "**Note:** The program opens a window containing the view from the camera. This must be closed in order to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# setup vision\n",
    "#################################################################################\n",
    "\n",
    "startpos = np.array([])\n",
    "end_pos  = np.array([])\n",
    "while not startpos.size or not end_pos.size:\n",
    "    success, img  = cap.read()\n",
    "    if success : \n",
    "        success2, true_map  = vis.get_clean_map(img, [field.xmax, field.ymax])\n",
    "        if success2 :\n",
    "            startpos  = vis.robot_detection(true_map)                                # try until we get valid positions\n",
    "            end_pos   = vis.goal_detection(true_map)  \n",
    "        else:\n",
    "            print(\"Could not make map\")\n",
    "    else :\n",
    "        print(\"No image captured\")\n",
    "    time.sleep(.1)   \n",
    "    \n",
    "true_map = vis.annotate_map(true_map, startpos, end_pos)\n",
    "cv2.imshow(\"True Map\", true_map)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#################################################################################\n",
    "# global path finding\n",
    "#################################################################################\n",
    "\n",
    "map_global_search = cv2.resize(vis.get_global_search_map(true_map, startpos), global_shape)# apply resizing to reduce computation cost of global search\n",
    "map_start         = util.rescale_int_coords(startpos, (true_map.shape[1], true_map.shape[0]), global_shape)  # starting point for global search\n",
    "map_end           = util.rescale_int_coords(end_pos,  (true_map.shape[1], true_map.shape[0]), global_shape)  # end point for global search\n",
    "\n",
    "occupancy_grid    = np.flipud(np.asarray(map_global_search))                     # make occupancy grid and change coordinate system           \n",
    "path              = PathPlanning.generate_global_path((map_start[1], map_start[0]), (map_end[1], map_end[0]), occupancy_grid)\n",
    "path              = util.adapt_pathfinder_coords(path, field, occupancy_grid.shape)\n",
    "\n",
    "if not path.size :\n",
    "    raise Exception(\"No Path Found\")\n",
    "\n",
    "#################################################################################\n",
    "# setup robot \n",
    "#################################################################################\n",
    "\n",
    "Tr = .2                                                                          # robot update timestep\n",
    "Tc = 2                                                                           # camera update timestep\n",
    "\n",
    "robot.xhat  = startpos.copy()                                                    # setup robot\n",
    "robot.xodo  = startpos.copy()\n",
    "robot.s     = np.diag([1, 1, .1, 20, 20])**2\n",
    "\n",
    "robot.d     = 90\n",
    "robot.set_Ts(Tr)\n",
    "robot.maxv  = 10\n",
    "robot.maxw  = np.pi/16\n",
    "\n",
    "field.goals = path\n",
    "field.xreal = startpos.copy()                                                    # prepare record of (mis)deeds\n",
    "field.xhat  = robot.xhat.copy()\n",
    "field.xodo  = robot.xodo.copy()\n",
    "field.s     = [2*np.sqrt(robot.s[0,0]+robot.s[1,1])]\n",
    "\n",
    "goals       = np.ndarray.tolist(field.goals)\n",
    "\n",
    "camera_pos.x = np.array([])                                                      # communication variable\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "# run\n",
    "#################################################################################\n",
    "T = 120                                                                           # maximum run time allowed\n",
    "\n",
    "rt = RepeatedTimer.Thread(Tr, robot_thread,  field, robot, goals, camera_pos);   # start threads\n",
    "ct = RepeatedTimer.Thread(Tc, camera_thread, cap, camera_pos);\n",
    "\n",
    "try:\n",
    "    for i in range(T):\n",
    "        time.sleep(1) \n",
    "        plt.clf();\n",
    "        plt.gcf().set_size_inches(12,8)\n",
    "        plt.imshow(occupancy_grid,cmap=colors.ListedColormap(['white', 'black']), origin = 'lower', extent=(field.xmin, field.xmax, field.ymin, field.ymax))\n",
    "        field.plot()\n",
    "        ds.display(plt.gcf())\n",
    "        ds.clear_output(wait=True)\n",
    "        \n",
    "        if not goals : \n",
    "            print(\"arrived\")\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    rt.stop()\n",
    "    ct.stop()\n",
    "    robot.th.set_var(\"event.args\", 2)\n",
    "    robot.do_motion(np.zeros((2,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to stop the robot and close the serial connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.do_motion(np.zeros((2,1)))\n",
    "robot.th.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Normal Run\n",
    "If the lighting conditions are good, we can generally achieve quite good results in a normal run, due to constant updates from the camera and continuous movement of the robot. \n",
    "\n",
    "A typical such run can be seen below. We plot the estimated position (red) against the position given by the camera (green) and the odometry only-based estimation (yellow). One can see that the odometry only-based estimation has a relatively large drift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\">\n",
       "<video width=\"80%\" controls>\n",
       "    <source src=\"./videos/run_2.mp4\">\n",
       "</video></div> <br>\n",
       "<i>Video 1: This video shows a successful test: The robot follows the planned path, while avoiding the local (white block) and global obstacles (black objects).</i>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "    <source src=\"./videos/run_2.mp4\">\n",
    "</video></div> <br>\n",
    "<i>Video 1: This video shows a successful test: The robot follows the planned path, while avoiding the local (white block) and global obstacles (black objects).</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Vision\n",
    "Since our system relies quite heaviliy on vision, we tested it when the vision is blocked. \n",
    "The vision can easily be blocked by covering one of the blue corner markers. In that case, no position is returned by the vision module. \n",
    "\n",
    "The result, while not perfect is acceptable. We plot the estimated position (red) against the position given by the camera (green) and the odometry only-based estimation (yellow). One can see that the odometry only-based estimation has a relatively large drift. The dotted blue lines represent the uncertainty of the robot as to its estimation, which is too optimistic, as the real position is far outside the 90% certainty zone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\">\n",
       "<video width=\"80%\" controls>\n",
       "    <source src=\"./videos/no_camera.mp4\">\n",
       "</video></div> <br>\n",
       "<i>Video 2: This video shows the robot successfully finding it’s path to the goal position, without using the camera (one of the blue sports is covered with a black sheet of paper during the global navigation)</i>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "    <source src=\"./videos/no_camera.mp4\">\n",
    "</video></div> <br>\n",
    "<i>Video 2: This video shows the robot successfully finding it’s path to the goal position, without using the camera (one of the blue sports is covered with a black sheet of paper during the global navigation)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kidnapping\n",
    "Since the localisation algorithm is a Kalman filter, it does not deal well with non-linearites and sudden jumps. \n",
    "One way to demonstrate this is to put the robot in a kidnapping scenario. \n",
    "\n",
    "The robot eventually recovers, but it takes some time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"middle\">\n",
       "<video width=\"80%\" controls>\n",
       "    <source src=\"./videos/kidnapping.mp4\">\n",
       "</video></div>\n",
       "<i>Video 3: This video shows the robot successfully dealing with a kidnapping situation. Also, the goal dot came unstuck.</i>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "    <source src=\"./videos/kidnapping.mp4\">\n",
    "</video></div>\n",
    "<i>Video 3: This video shows the robot successfully dealing with a kidnapping situation. Also, the goal dot came unstuck.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing\n",
    "To summarise, image processing provides quite good results in regards with the methods tested. There are still failures in localising the robot : in hindsight, the red color wasn't such a good choice due to the red LEDs on the robot. There are also some uncertainties due the light (intensity and color) and shadows. In general some robustness steps have been implemented, but the speed of the algorithm could be much better. We aimed for a robust rather than a fast method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Path Planning\n",
    "\n",
    "The global path finding part returns successively the shortest path from the start to goal. But the implemented A* algorithm uses a not negligible amount of time and storage space to fulfill this task. Nevertheless the A* algorithm seems to be sufficient for our task, because it is only executed once in the beginning of the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localisation and Control\n",
    "The employed localisation method, a Kalman estimator on the position of the robot does not work very well. The non-linearities of the system make it such that a gaussian probability density function does not correctly represent the uncertainty.\n",
    "\n",
    "Possible improvements which were considered, but not investigated due to lack of time, are:\n",
    "- using a Kalman filter in the $[dl, d\\phi]^{T}$ space, where the robot's motion model is linear\n",
    "- using a modified filter and multiple gaussians to better represent the true probablilty distribution\n",
    "- using a particle filter (this would make the system very slow though)\n",
    "\n",
    "However, thanks to the use of frequent position updated from the camera, we were able to keep the uncertainties low and thus the probabiltiy density function *nearly* gaussian, and thus achieve an acceptable overall result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Navigation\n",
    "\n",
    "To get the local navigation to work, the parameters had to be tweaked which lead to extensive testing. The hand-off between Python and Aseba did not work well at first. However, after letting the robot get out of the local obstacle avoidance mode only if the obstacle was not between the robot and the goal, the switching between local and global navigation was successful. Differences in lighting conditions for different group members meant that the robot would sometimes get too close to the obstacle. Thus the robot would bump into the obstacle. The solution of this issue was to adjust the parameters. \n",
    "\n",
    "Due to time constraints, avoiding global obstacles during the local navigation was not implemented. However, if the local obstacle is not placed next to a global obstacle, this is not a problem and the local navigation works fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall\n",
    "\n",
    "Due to the current COVID-19 pandemic and the geographical dipersion of the group members, all collaboration was done online. \n",
    "This unfortunately made combining the modules, testing and debugging them extremely time-consuming, especially since only one \"environment\" was available.\n",
    "\n",
    "We are therefore quite satisfied with the results of this project, though of course, had we had the time, we would have liked to improve on a number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
